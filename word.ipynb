{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The oneperfectly divine thing, the oneglimpse of God’s paradisegiven on earth, is to fight a losingbattle - and notlose it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['The','one','perfectly','divine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "locs = list(set([(m.start(),m.end()) for word in word_list for m in re.finditer(word, text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 23), (7, 16), (0, 3), (35, 38), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts = [m.start() for m in re.finditer(' ', text)]\n",
    "spacestarts.append(-1)\n",
    "spacestarts.append(len(text))\n",
    "spacestarts.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacestarts_affine = [ss+1 for ss in spacestarts]\n",
    "spacestarts_affine.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces = [(spacestarts[k] + 1,spacestarts[k + 1]) for k in range(0,len(spacestarts) - 1 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 16), (24, 30), (31, 34), (35, 45), (46, 48), (49, 54), (55, 68), (69, 71), (72, 78), (79, 81), (82, 84), (85, 90), (91, 92), (93, 105), (106, 107), (108, 111), (112, 119), (120, 123)]\n"
     ]
    }
   ],
   "source": [
    "print(between_spaces_notvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Антон\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "wordlist = set(brown.words())\n",
    "word_list = list(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word.replace('*','') for word in word_list]\n",
    "word_list = [word.replace('[','') for word in word_list]\n",
    "word_list = [word.replace(']','') for word in word_list]\n",
    "word_list = [word.replace('?','') for word in word_list]\n",
    "word_list = [word.replace('.','') for word in word_list]\n",
    "word_list = [word.replace('+','') for word in word_list]\n",
    "word_list = [word.replace('/','') for word in word_list]\n",
    "word_list = [word.replace(';','') for word in word_list]\n",
    "word_list = [word.replace(':','') for word in word_list]\n",
    "word_list = [word.replace(',','') for word in word_list]\n",
    "word_list = [word.replace(')','') for word in word_list]\n",
    "word_list = [word.replace('(','') for word in word_list]\n",
    "word_list.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 16), (24, 30), (35, 45), (49, 54), (55, 68), (72, 78), (93, 105), (112, 119), (120, 123)]\n"
     ]
    }
   ],
   "source": [
    "print(between_spaces_notvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_words = [loc for loc in locs if loc[0] in spacestarts_affine and loc[1] not in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affine and loc[1] in spacestarts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = between_spaces_notvalid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "endsofbeginnings = [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1] - loc[0]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1] - loc[0]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = list(set(endsofbeginnings).intersection(beginningsofends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pivot = np.min(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one perfectly divine thing, the oneglimpse of God’s paradisegiven on earth, is to fight a losingbattle - and notlose it.\n"
     ]
    }
   ],
   "source": [
    "textnew = text\n",
    "textnew = textnew.replace(text[loc[0]:loc[1]], text[loc[0]:pivot] + ' ' + text[pivot:loc[1]])\n",
    "print(textnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertspaces(text,word_list):\n",
    "    \"\"\"Функция для вставки пробелов в текст\"\"\"\n",
    "    locs = list(set([(m.start(),m.end()) for word in word_list for m in re.finditer(word, text)]))\n",
    "    \n",
    "    spacestarts = [m.start() for m in re.finditer(' ', text)]\n",
    "    spacestarts.append(-1)\n",
    "    spacestarts.append(len(text))\n",
    "    spacestarts.sort()\n",
    "    spacestarts_affine = [ss + 1 for ss in spacestarts]\n",
    "    spacestarts_affine.sort()\n",
    "    \n",
    "    partial_words = [loc for loc in locs if loc[0] in spacestarts_affine and loc[1] not in spacestarts]\n",
    "    partial_words_end = [loc for loc in locs if loc[0] not in spacestarts_affine and loc[1] in spacestarts]\n",
    "    \n",
    "    between_spaces = [(spacestarts[k] + 1,spacestarts[k+1]) for k in range(0,len(spacestarts) - 1)]\n",
    "    between_spaces_notvalid = [loc for loc in between_spaces if text[loc[0]:loc[1]] not in word_list]\n",
    "    \n",
    "    textnew = text\n",
    "    \n",
    "    for loc in between_spaces_notvalid:\n",
    "        endsofbeginnings = [loc2[1] for loc2 in partial_words if loc2[0] == loc[0] and (loc2[1] - loc[0]) > 1]\n",
    "        beginningsofends = [loc2[0] for loc2 in partial_words_end if loc2[1] == loc[1] and (loc2[1] - loc[0]) > 1]\n",
    "        \n",
    "        pivot = list(set(endsofbeginnings).intersection(beginningsofends))\n",
    "        \n",
    "        if len(pivot) > 0:\n",
    "            pivot = np.min(pivot)\n",
    "            textnew = textnew.replace(text[loc[0]:loc[1]], text[loc[0]:pivot] + ' '+ text[pivot:loc[1]])\n",
    "    \n",
    "    textnew = textnew.replace('  ',' ')\n",
    "    return textnew\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The one perfectly divine thing, the one glimpse of God's paradise given on earth, is to fight a losing battle - and not lose it.\n"
     ]
    }
   ],
   "source": [
    "text = \"The oneperfectly divine thing, the oneglimpse of God's paradisegiven on earth, is to fight a losingbattle - and notlose it.\"\n",
    "print(insertspaces(text,word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'forks', 'perpetually', 'toward', 'innumerable', 'futures']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Time forks perpetually toward innumerable futures\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = [ngrams(token,2),ngrams(token,3),ngrams(token,4),ngrams(token,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "file = requests.get('http://www.bradfordtuckfield.com/shakespeare.txt')\n",
    "file = file.text\n",
    "text = file.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "grams = [ngrams(token,2),ngrams(token,3),ngrams(token,4),ngrams(token,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "search_term = 'life is a'\n",
    "split_term = tuple(search_term.split(' '))\n",
    "search_term_length = len(search_term.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counted_grams = Counter(grams[search_term_length - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('From', 'fairest', 'creatures', 'we'), 1)\n"
     ]
    }
   ],
   "source": [
    "print(list(counted_grams.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_suggestion(search_term, text):\n",
    "    \"\"\"\n",
    "    Функция, предоставляющая поисковые рекомендации; для этого она берет\n",
    "    n-грамму и возвращает наиболее вероятную (n + 1)-грамму, начинающуюся с заданной n-граммы\n",
    "    \"\"\"\n",
    "    token = nltk.word_tokenize(text)\n",
    "    \n",
    "    ngrams(token,2)\n",
    "    ngrams(token,3)\n",
    "    ngrams(token,4)\n",
    "    ngrams(token,5)\n",
    "    \n",
    "    grams = [ngrams(token,2),ngrams(token,3),ngrams(token,4),ngrams(token,5)]\n",
    "    split_term = tuple(search_term.split(' '))\n",
    "    search_term_length = len(search_term.split(' '))\n",
    "    counted_grams = Counter(grams[search_term_length-1])\n",
    "    combined_term = 'No suggested searches'\n",
    "    matching_terms = [element for element in list(counted_grams.items()) if element[0][:-1] == tuple(split_term)]\n",
    "    \n",
    "    if len(matching_terms) > 0:\n",
    "        frequencies = [item[1] for item in matching_terms]\n",
    "        maximum_frequency = np.max(frequencies)\n",
    "        highest_frequency_term = [item[0] for item in matching_terms if item[1] == maximum_frequency][0]\n",
    "        combined_term = ' '.join(highest_frequency_term)\n",
    "        \n",
    "    return combined_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is a tedious\n"
     ]
    }
   ],
   "source": [
    "file = requests.get('http://www.bradfordtuckfield.com/shakespeare.txt')\n",
    "file = file=file.text\n",
    "text = file.replace('\\n', '')\n",
    "print(search_suggestion('life is a', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life is a failure\n"
     ]
    }
   ],
   "source": [
    "file = requests.get('http://www.bradfordtuckfield.com/marktwain.txt')\n",
    "file = file=file.text\n",
    "text = file.replace('\\n', '')\n",
    "print(search_suggestion('life is a',text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
